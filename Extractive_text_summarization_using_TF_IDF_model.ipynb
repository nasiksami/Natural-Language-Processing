{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Extractive_text_summarization_using_TF_IDF_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nasiksami/Natural-Language-Processing/blob/main/Extractive_text_summarization_using_TF_IDF_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1wJlg-m22H6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "196884a4-e590-4adf-88b9-c88c606ee471"
      },
      "source": [
        "!pip install rouge"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rouge\n",
            "  Downloading https://files.pythonhosted.org/packages/43/cc/e18e33be20971ff73a056ebdb023476b5a545e744e3fc22acd8c758f1e0d/rouge-1.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rouge) (1.15.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEqWgtIQiaP0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5c2416e2-a701-4e55-b1ee-818435764202"
      },
      "source": [
        "#importing all necessary libraries\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import reuters\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk.data\n",
        "import math\n",
        "import re\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from rouge import Rouge \n",
        "nltk.download('all')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJLZctTCkcSh"
      },
      "source": [
        "SUMMARY_LENGTH = 5  # number of sentences in final summary\n",
        "\n",
        "stop_words = stopwords.words('english') #getting all the stopwords\n",
        "ideal_sent_length = 20.0\n",
        "stemmer = PorterStemmer()  #using PorterStemmer for stemming purposes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKLzFRRIlXMY"
      },
      "source": [
        "class Summarizer():\n",
        "\n",
        "    #taking in two articles and extracting the headline and body to place them in a list\n",
        "    def __init__(self, article):\n",
        "      self._articles = []\n",
        "      for doc in article:\n",
        "            with open(doc) as f:\n",
        "                headline = f.readline() #first line of article is the headline\n",
        "                url = f.readline() #second line of article is the url\n",
        "                body = f.read().replace('\\n', ' ') #read the remaining of the article and replace the empty lines with a whitespace\n",
        "                #if headline and body is not empty, then we assign the values into 'articles' list\n",
        "                if not self.valid_input(headline, body):\n",
        "                    self._articles.append((None, None))\n",
        "                    continue\n",
        "                self._articles.append((headline, body))\n",
        " \n",
        "    \n",
        "    #check if headline and body has any text or not\n",
        "    def valid_input(self, headline, article_text):\n",
        "        return headline != '' and article_text != ''\n",
        "\n",
        "\n",
        "    #function for generating the summary\n",
        "    def generate_summaries(self):\n",
        "        \n",
        "        #Identifying the number of sentences in the list (containing two articles)\n",
        "        total_num_sentences = 0\n",
        "        for article in self._articles:\n",
        "            total_num_sentences += len(self.split_into_sentences(article[1]))\n",
        "        \n",
        "        #If article is shorter than the desired summary, just return the original articles\n",
        "        if total_num_sentences <= SUMMARY_LENGTH:\n",
        "            return [x[1] for x in self._articles]\n",
        "\n",
        "        self.build_TFIDF_model()  #build tf-idf score matrix from Reuters corpus\n",
        "\n",
        "        self._scores = Counter()\n",
        "        for article in self._articles:\n",
        "            self.score(article)\n",
        "\n",
        "        highest_scoring = self._scores.most_common(SUMMARY_LENGTH)\n",
        "\n",
        "        #print(\"## Headlines: \")\n",
        "        #for article in self._articles:\n",
        "        #    print(\"- \" + article[0])\n",
        "\n",
        "        #print(\"\\n## Summary: \")\n",
        "        # Appends highest scoring \"representative\" sentences, returns as a single summary paragraph.\n",
        "        #file to save the result\n",
        "        \n",
        "        with open(\"result.txt\", \"w\") as fwrite:\n",
        "          fwrite.write(\"## Headlines: \\n\")\n",
        "          for article in self._articles:\n",
        "            fwrite.write(\"- \")\n",
        "            fwrite.write(article[0])\n",
        "          fwrite.write(\"\\n## Summary: \\n\")\n",
        "          fwrite.write(' '.join([sent[0] for sent in highest_scoring])) \n",
        "        return ' '.join([sent[0] for sent in highest_scoring])\n",
        "\n",
        "    \n",
        "    \n",
        "    #building tf_idf score in a matrix for each word in the Reuters corpus\n",
        "    def build_TFIDF_model(self):\n",
        "        token_dict = {}\n",
        "        #getiing the name of each article and its content\n",
        "        for article in reuters.fileids():\n",
        "            token_dict[article] = reuters.raw(article) \n",
        "\n",
        "        # Use TF-IDF to determine frequency of each word in our article, relative to the\n",
        "        # word frequency distributions in corpus of 10.8k Reuters news articles.\n",
        "        self._tfidf = TfidfVectorizer(tokenizer=self.tokenize_and_stem, stop_words='english', decode_error='ignore')\n",
        "        tdm = self._tfidf.fit_transform(token_dict.values())  # Term-document matrix\n",
        "        \n",
        "\n",
        "    #perform tokenization and stemming (using PorterStemmer)  \n",
        "    def tokenize_and_stem(self, text):\n",
        "        tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
        "        filtered = []\n",
        "\n",
        "        #filter out numeric tokens, raw punctuation, etc.\n",
        "        for token in tokens:\n",
        "            if re.search('[a-zA-Z]', token):\n",
        "                filtered.append(token)\n",
        "        stems = [stemmer.stem(t) for t in filtered]\n",
        "        return stems\n",
        "\n",
        " \n",
        "    #Assigns each sentence in the document a score based on the sum of features values.\n",
        "    #Based on 4 features: relevance to headline, length, sentence position, and TF*IDF frequency.\n",
        "    def score(self, article):\n",
        "        \n",
        "        headline = article[0]\n",
        "        sentences = self.split_into_sentences(article[1])\n",
        "        frequency_scores = self.frequency_scores(article[1])\n",
        "\n",
        "        for i, s in enumerate(sentences):\n",
        "            headline_score = self.headline_score(headline, s) * 1.5\n",
        "            length_score = self.length_score(self.split_into_words(s)) * 1.0\n",
        "            position_score = self.position_score(float(i+1), len(sentences)) * 1.0\n",
        "            frequency_score = frequency_scores[i] * 4\n",
        "            score = (headline_score + frequency_score + length_score + position_score) / 4.0\n",
        "            self._scores[s] = score\n",
        "\n",
        "     \n",
        "    #extracting each sentence\n",
        "    def split_into_sentences(self, text):\n",
        "        #identify where each sentence ends\n",
        "        tok = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "        sentences = tok.tokenize(self.remove_smart_quotes(text))\n",
        "        #consider sentences with a length of more than 10\n",
        "        sentences = [sent.replace('\\n', '') for sent in sentences if len(sent) > 10]\n",
        "        return sentences\n",
        "    \n",
        "    #identify and remove smart quotes\n",
        "    def remove_smart_quotes(self, text):\n",
        "        return text.replace(u\"\\u201c\",\"\").replace(u\"\\u201d\", \"\")\n",
        "\n",
        "\n",
        "    #Individual (stemmed) word weights are then calculated for each\n",
        "    #word in the given article. Sentences are scored as the sum of their TF-IDF word frequencies.    \n",
        "    def frequency_scores(self, article_text):\n",
        "\n",
        "        # Add our document into the model so we can retrieve scores\n",
        "        response = self._tfidf.transform([article_text])\n",
        "        feature_names = self._tfidf.get_feature_names() # these are just stemmed words\n",
        "\n",
        "        word_prob = {}  # TF-IDF individual word probabilities\n",
        "        for col in response.nonzero()[1]:\n",
        "            word_prob[feature_names[col]] = response[0, col]\n",
        "\n",
        "        #Taking each sentence score based on its word probability\n",
        "        sent_scores = []\n",
        "        for sentence in self.split_into_sentences(article_text):\n",
        "            score = 0\n",
        "            sent_tokens = self.tokenize_and_stem(sentence)\n",
        "            for token in (t for t in sent_tokens if t in word_prob):\n",
        "                score += word_prob[token]\n",
        "\n",
        "            # Normalize score by length of sentence, since we later factor in sentence length as a feature\n",
        "            sent_scores.append(score / len(sent_tokens))\n",
        "\n",
        "        return sent_scores\n",
        "\n",
        "    #Gives sentence a score between 0 to 1 based on percentage of words common to the headline. \n",
        "    def headline_score(self, headline, sentence):\n",
        "        title_stems = [stemmer.stem(w) for w in headline if w not in stop_words]\n",
        "        sentence_stems = [stemmer.stem(w) for w in sentence if w not in stop_words]\n",
        "        count = 0.0\n",
        "        for word in sentence_stems:\n",
        "            if word in title_stems:\n",
        "                count += 1.0\n",
        "        score = count / len(title_stems)\n",
        "        return score\n",
        "  \n",
        "    #Gives sentence score between (0,1) based on how close sentence's length is to the ideal length. \n",
        "    def length_score(self, sentence):\n",
        "        len_diff = math.fabs(ideal_sent_length - len(sentence))\n",
        "        return len_diff / ideal_sent_length\n",
        "      \n",
        "      \n",
        "    #Split a sentence string into an array of words\n",
        "    def split_into_words(self, text):\n",
        "        try:\n",
        "            text = re.sub(r'[^\\w ]', '', text) # remove non-words\n",
        "            return [w.strip('.').lower() for w in text.split()]\n",
        "        except TypeError:\n",
        "            return None\n",
        "\n",
        "    #Yields a value between (0,1), corresponding to sentence's position in the article.\n",
        "    #Assuming that sentences at the very beginning and ends of the article have a higher weight. \n",
        "    #Values borrowed from https://github.com/xiaoxu193/PyTeaser  \n",
        "    def position_score(self, i, size):\n",
        "\n",
        "        relative_position = i / size\n",
        "        if 0 < relative_position <= 0.1:\n",
        "            return 0.17\n",
        "        elif 0.1 < relative_position <= 0.2:\n",
        "            return 0.23\n",
        "        elif 0.2 < relative_position <= 0.3:\n",
        "            return 0.14\n",
        "        elif 0.3 < relative_position <= 0.4:\n",
        "            return 0.08\n",
        "        elif 0.4 < relative_position <= 0.5:\n",
        "            return 0.05\n",
        "        elif 0.5 < relative_position <= 0.6:\n",
        "            return 0.04\n",
        "        elif 0.6 < relative_position <= 0.7:\n",
        "            return 0.06\n",
        "        elif 0.7 < relative_position <= 0.8:\n",
        "            return 0.04\n",
        "        elif 0.8 < relative_position <= 0.9:\n",
        "            return 0.04\n",
        "        elif 0.9 < relative_position <= 1.0:\n",
        "            return 0.15\n",
        "        else:\n",
        "            return 0   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9IBeKvBuheu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "617ee42b-0c6d-409b-e76d-8d703193f33b"
      },
      "source": [
        "clmt_articles = [\"climate-nyt.txt\", \"climate-npr.txt\"] #getting two articles into the list\n",
        "clmt_sum = Summarizer(clmt_articles) #Passing those two articles into the 'Summarizer' class\n",
        "s = clmt_sum.generate_summaries() #Print the summary\n",
        "print(s)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Despite a domestic agenda consumed by his response to last week’s terrorist attack in California, President Obama is closely tracking the United Nations climate change negotiations outside Paris, Secretary of State John Kerry said in an interview on Tuesday. Though the foundation of the Saudi economy could be threatened by an accord intended to turn the world away from fossil fuels like oil, Saudi Arabia is a desert nation that could also experience more devastating heat as a result of global warming. The United States and China have jointly pledged to cut greenhouse gas emissions, but the United States has been pushing for aggressive terms requiring countries to verify their reduced emissions in a step that the Chinese see as intrusive. The president spoke by phone with Prime Minister Narendra Modi of India on Tuesday about an agreement under consideration here and conferred by phone this week with President Dilma Rousseff of Brazil, Mr. Kerry said. Some countries at the summit accuse the U.S. — which, in the 20th century, has emitted more carbon than any other — of trying to have it both ways: emitting more carbon per capita than almost any other country, while wagging fingers at the rest of the world.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIfccE2awefG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "cf1b2a11-fc40-4229-95a6-2d64362a945c"
      },
      "source": [
        "#http://textsummarization.net/text-summarizer\n",
        "\n",
        "#summary length 5\n",
        "r = 'LE BOURGET, France — Despite a domestic agenda consumed by his response to last week’s terrorist attack in California, President Obama is closely tracking the United Nations climate change negotiations outside Paris, Secretary of State John Kerry said in an interview on Tuesday. The president spoke by phone with Prime Minister Narendra Modi of India on Tuesday about an agreement under consideration here and conferred by phone this week with President Dilma Rousseff of Brazil, Mr. Kerry said. India and Brazil, along with China and the United States, are among the world’s top producers of greenhouse gases that warm the planet. The role of the four nations in reaching a deal to curb those emissions is crucial to the success of the negotiations.'\n",
        "\n",
        "#summary length 3\n",
        "#r = 'LE BOURGET, France — Despite a domestic agenda consumed by his response to last week’s terrorist attack in California, President Obama is closely tracking the United Nations climate change negotiations outside Paris, Secretary of State John Kerry said in an interview on Tuesday. The role of the four nations in reaching a deal to curb those emissions is crucial to the success of the negotiations. India and Brazil, along with China and the United States, are among the world’s top producers of greenhouse gases that warm the planet.'\n",
        "\n",
        "#summary length 8\n",
        "#r = 'LE BOURGET, France — Despite a domestic agenda consumed by his response to last week’s terrorist attack in California, President Obama is closely tracking the United Nations climate change negotiations outside Paris, Secretary of State John Kerry said in an interview on Tuesday. The president spoke by phone with Prime Minister Narendra Modi of India on Tuesday about an agreement under consideration here and conferred by phone this week with President Dilma Rousseff of Brazil, Mr. Kerry said. India and Brazil, along with China and the United States, are among the world’s top producers of greenhouse gases that warm the planet. The role of the four nations in reaching a deal to curb those emissions is crucial to the success of the negotiations.'\n",
        "\n",
        "print('Individual 1-gram: %f' % sentence_bleu(r, s, weights=(1, 0, 0, 0)))\n",
        "print('Individual 2-gram: %f' % sentence_bleu(r, s, weights=(0, 1, 0, 0)))\n",
        "print('Individual 3-gram: %f' % sentence_bleu(r, s, weights=(0, 0, 1, 0)))\n",
        "print('Individual 4-gram: %f' % sentence_bleu(r, s, weights=(0, 0, 0, 1)))\n",
        "\n",
        "#The weights for the BLEU-4 are 1/4 (25%) or 0.25 for each of the 1-gram, 2-gram, 3-gram and 4-gram scores\n",
        "score = sentence_bleu(r, s, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "print('BLEU 4 Score: ', score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Individual 1-gram: 0.034314\n",
            "Individual 2-gram: 1.000000\n",
            "Individual 3-gram: 1.000000\n",
            "Individual 4-gram: 1.000000\n",
            "BLEU 4 Score:  0.430394752998613\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eD3wuRX0_2x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "6478ca8a-9a61-43ba-f5ff-7c29c3195493"
      },
      "source": [
        "#https://www.tools4noobs.com/summarize/\n",
        "r = 'LE BOURGET, France — Despite a domestic agenda consumed by his response to last week’s terrorist attack in California, President Obama is closely tracking the United Nations climate change negotiations outside Paris, Secretary of State John Kerry said in an interview on Tuesday.'\n",
        "\n",
        "print('Individual 1-gram: %f' % sentence_bleu(r, s, weights=(1, 0, 0, 0)))\n",
        "print('Individual 2-gram: %f' % sentence_bleu(r, s, weights=(0, 1, 0, 0)))\n",
        "print('Individual 3-gram: %f' % sentence_bleu(r, s, weights=(0, 0, 1, 0)))\n",
        "print('Individual 4-gram: %f' % sentence_bleu(r, s, weights=(0, 0, 0, 1)))\n",
        "\n",
        "#The weights for the BLEU-4 are 1/4 (25%) or 0.25 for each of the 1-gram, 2-gram, 3-gram and 4-gram scores\n",
        "score = sentence_bleu(r, s, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "print('BLEU 4 Score: ', score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Individual 1-gram: 0.031863\n",
            "Individual 2-gram: 1.000000\n",
            "Individual 3-gram: 1.000000\n",
            "Individual 4-gram: 1.000000\n",
            "BLEU 4 Score:  0.4224942447227585\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JbvV_il2CTH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "4b9a07b8-8541-4a90-f2af-76c37af84e75"
      },
      "source": [
        "#http://textsummarization.net/text-summarizer\n",
        "\n",
        "#summary length 5\n",
        "r = 'LE BOURGET, France — Despite a domestic agenda consumed by his response to last week’s terrorist attack in California, President Obama is closely tracking the United Nations climate change negotiations outside Paris, Secretary of State John Kerry said in an interview on Tuesday. The president spoke by phone with Prime Minister Narendra Modi of India on Tuesday about an agreement under consideration here and conferred by phone this week with President Dilma Rousseff of Brazil, Mr. Kerry said. India and Brazil, along with China and the United States, are among the world’s top producers of greenhouse gases that warm the planet. The role of the four nations in reaching a deal to curb those emissions is crucial to the success of the negotiations.'\n",
        "\n",
        "#summary length 3\n",
        "#r = 'LE BOURGET, France — Despite a domestic agenda consumed by his response to last week’s terrorist attack in California, President Obama is closely tracking the United Nations climate change negotiations outside Paris, Secretary of State John Kerry said in an interview on Tuesday. The role of the four nations in reaching a deal to curb those emissions is crucial to the success of the negotiations. India and Brazil, along with China and the United States, are among the world’s top producers of greenhouse gases that warm the planet.'\n",
        "\n",
        "#summary length 8\n",
        "#r = 'LE BOURGET, France — Despite a domestic agenda consumed by his response to last week’s terrorist attack in California, President Obama is closely tracking the United Nations climate change negotiations outside Paris, Secretary of State John Kerry said in an interview on Tuesday. The president spoke by phone with Prime Minister Narendra Modi of India on Tuesday about an agreement under consideration here and conferred by phone this week with President Dilma Rousseff of Brazil, Mr. Kerry said. India and Brazil, along with China and the United States, are among the world’s top producers of greenhouse gases that warm the planet. The role of the four nations in reaching a deal to curb those emissions is crucial to the success of the negotiations.'\n",
        "\n",
        "rouge = Rouge()\n",
        "scores = rouge.get_scores(s, r)\n",
        "\n",
        "print(scores)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'rouge-1': {'f': 0.5665236004494466, 'p': 0.45517241379310347, 'r': 0.75}, 'rouge-2': {'f': 0.45016076701068025, 'p': 0.36082474226804123, 'r': 0.5982905982905983}, 'rouge-l': {'f': 0.5013234240282473, 'p': 0.4482758620689655, 'r': 0.7386363636363636}}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTj77urc3Tcp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "81b48fda-fd58-4eed-b6ce-e892cc7e37bd"
      },
      "source": [
        "#https://www.tools4noobs.com/summarize/\n",
        "r = 'LE BOURGET, France — Despite a domestic agenda consumed by his response to last week’s terrorist attack in California, President Obama is closely tracking the United Nations climate change negotiations outside Paris, Secretary of State John Kerry said in an interview on Tuesday.'\n",
        "\n",
        "rouge = Rouge()\n",
        "scores = rouge.get_scores(s, r)\n",
        "print(scores)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'rouge-1': {'f': 0.4171122959821556, 'p': 0.2689655172413793, 'r': 0.9285714285714286}, 'rouge-2': {'f': 0.3220338953792014, 'p': 0.1958762886597938, 'r': 0.9047619047619048}, 'rouge-l': {'f': 0.28461501265052624, 'p': 0.2689655172413793, 'r': 0.9285714285714286}}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyoENx10rgAL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lT3z6z64hAu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DAQX24s4hGq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "049QTxZY4hJw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zc15agqM4hPA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIcq0Ein4hSd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oBOEaSg4hNH"
      },
      "source": [
        "import math\n",
        "\n",
        "from nltk import sent_tokenize, word_tokenize, PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from sys import argv\n",
        "    \n",
        "\n",
        "def _create_frequency_matrix(sentences):\n",
        "    frequency_matrix = {}\n",
        "    stopWords = set(stopwords.words(\"english\"))\n",
        "    ps = PorterStemmer()\n",
        "\n",
        "    for sent in sentences:\n",
        "        freq_table = {}\n",
        "        words = word_tokenize(sent)\n",
        "        for word in words:\n",
        "            word = word.lower()\n",
        "            word = ps.stem(word)\n",
        "            if word in stopWords:\n",
        "                continue\n",
        "\n",
        "            if word in freq_table:\n",
        "                freq_table[word] += 1\n",
        "            else:\n",
        "                freq_table[word] = 1\n",
        "\n",
        "        frequency_matrix[sent[:15]] = freq_table\n",
        "\n",
        "    return frequency_matrix\n",
        "\n",
        "def _create_tf_matrix(freq_matrix):\n",
        "    tf_matrix = {}\n",
        "\n",
        "    for sent, f_table in freq_matrix.items():\n",
        "        tf_table = {}\n",
        "\n",
        "        count_words_in_sentence = len(f_table)\n",
        "        for word, count in f_table.items():\n",
        "            tf_table[word] = count / count_words_in_sentence\n",
        "\n",
        "        tf_matrix[sent] = tf_table\n",
        "\n",
        "    return tf_matrix\n",
        "\n",
        "def _create_documents_per_words(freq_matrix):\n",
        "    word_per_doc_table = {}\n",
        "\n",
        "    for sent, f_table in freq_matrix.items():\n",
        "        for word, count in f_table.items():\n",
        "            if word in word_per_doc_table:\n",
        "                word_per_doc_table[word] += 1\n",
        "            else:\n",
        "                word_per_doc_table[word] = 1\n",
        "\n",
        "    return word_per_doc_table\n",
        "\n",
        "def _create_idf_matrix(freq_matrix, count_doc_per_words, total_documents):\n",
        "    idf_matrix = {}\n",
        "\n",
        "    for sent, f_table in freq_matrix.items():\n",
        "        idf_table = {}\n",
        "\n",
        "        for word in f_table.keys():\n",
        "            idf_table[word] = math.log10(total_documents / float(count_doc_per_words[word]))\n",
        "\n",
        "        idf_matrix[sent] = idf_table\n",
        "\n",
        "    return idf_matrix\n",
        "\n",
        "def _create_tf_idf_matrix(tf_matrix, idf_matrix):\n",
        "    tf_idf_matrix = {}\n",
        "\n",
        "    for (sent1, f_table1), (sent2, f_table2) in zip(tf_matrix.items(), idf_matrix.items()):\n",
        "\n",
        "        tf_idf_table = {}\n",
        "\n",
        "        for (word1, value1), (word2, value2) in zip(f_table1.items(),\n",
        "                                                    f_table2.items()):  # here, keys are the same in both the table\n",
        "            tf_idf_table[word1] = float(value1 * value2)\n",
        "\n",
        "        tf_idf_matrix[sent1] = tf_idf_table\n",
        "\n",
        "    return tf_idf_matrix\n",
        "\n",
        "def _score_sentences(tf_idf_matrix) -> dict:\n",
        "    \"\"\"\n",
        "    score a sentence by its word's TF\n",
        "    Basic algorithm: adding the TF frequency of every non-stop word in a sentence divided by total no of words in a sentence.\n",
        "    :rtype: dict\n",
        "    \"\"\"\n",
        "\n",
        "    sentenceValue = {}\n",
        "\n",
        "    for sent, f_table in tf_idf_matrix.items():\n",
        "        total_score_per_sentence = 0\n",
        "\n",
        "        count_words_in_sentence = len(f_table)\n",
        "        for word, score in f_table.items():\n",
        "            total_score_per_sentence += score\n",
        "\n",
        "        sentenceValue[sent] = total_score_per_sentence / count_words_in_sentence\n",
        "\n",
        "    return sentenceValue\n",
        "\n",
        "def _find_average_score(sentenceValue) -> int:\n",
        "    \"\"\"\n",
        "    Find the average score from the sentence value dictionary\n",
        "    :rtype: int\n",
        "    \"\"\"\n",
        "    sumValues = 0\n",
        "    for entry in sentenceValue:\n",
        "        sumValues += sentenceValue[entry]\n",
        "\n",
        "    # Average value of a sentence from original summary_text\n",
        "    average = (sumValues / len(sentenceValue))\n",
        "\n",
        "    return average\n",
        "\n",
        "def _generate_summary(sentences, sentenceValue, threshold):\n",
        "    sentence_count = 0\n",
        "    summary = ''\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if sentence[:15] in sentenceValue and sentenceValue[sentence[:15]] >= (threshold):\n",
        "            summary += \" \" + sentence\n",
        "            sentence_count += 1\n",
        "\n",
        "    return summary\n",
        "\n",
        "\n",
        "'''\n",
        "We already have a sentence tokenizer, so we just need \n",
        "to run the sent_tokenize() method to create the array of sentences.\n",
        "'''\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    data=open(argv[1],\"r\")\n",
        "    sentences = sent_tokenize(f.read())\n",
        "\n",
        "\n",
        "    print(\"\\n\\n\\n\")\n",
        "    # 1 Sentence Tokenize\n",
        "    \n",
        "    total_documents = len(sentences)\n",
        "    #print(sentences)\n",
        "\n",
        "    # 2 Create the Frequency matrix of the words in each sentence.\n",
        "    freq_matrix = _create_frequency_matrix(sentences)\n",
        "    #print(freq_matrix)\n",
        "\n",
        "\n",
        "    # 3 Calculate TermFrequency and generate a matrix\n",
        "    tf_matrix = _create_tf_matrix(freq_matrix)\n",
        "    #print(tf_matrix)\n",
        "\n",
        "    # 4 creating table for documents per words\n",
        "    count_doc_per_words = _create_documents_per_words(freq_matrix)\n",
        "    #print(count_doc_per_words)\n",
        "\n",
        "\n",
        "\n",
        "    # 5 Calculate IDF and generate a matrix\n",
        "    idf_matrix = _create_idf_matrix(freq_matrix, count_doc_per_words, total_documents)\n",
        "    #print(idf_matrix)\n",
        "\n",
        "    # 6 Calculate TF-IDF and generate a matrix\n",
        "    tf_idf_matrix = _create_tf_idf_matrix(tf_matrix, idf_matrix)\n",
        "    #print(tf_idf_matrix)\n",
        "\n",
        "    # 7 Important Algorithm: score the sentences\n",
        "    sentence_scores = _score_sentences(tf_idf_matrix)\n",
        "    #print(sentence_scores)\n",
        "\n",
        "    # 8 Find the threshold\n",
        "    threshold = _find_average_score(sentence_scores)\n",
        "    #print(threshold)\n",
        "\n",
        "    # 9 Important Algorithm: Generate the summary\n",
        "    summary = _generate_summary(sentences, sentence_scores, 1.3 * threshold)\n",
        "    print(summary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3B4dVDRy4iDy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}